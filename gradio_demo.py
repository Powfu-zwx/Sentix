#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æƒ…æ„ŸAIåŠ©æ‰‹ - Gradioç½‘é¡µç•Œé¢
é›†æˆæƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½
"""

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM

class EmotionAI:
    def __init__(self):
        self.sentiment_model = None
        self.sentiment_tokenizer = None
        self.generation_model = None
        self.generation_tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.load_models()
    
    def load_models(self):
        """åŠ è½½æƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        try:
            # åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹
            sentiment_model_path = './sentiment_model'
            print("åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
            self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path)
            self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path)
            self.sentiment_model = self.sentiment_model.to(self.device)
            self.sentiment_model.eval()
            
            # åŠ è½½æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
            generation_model_name = "uer/gpt2-chinese-cluecorpussmall"
            print("åŠ è½½æ–‡æœ¬ç”Ÿæˆæ¨¡å‹...")
            self.generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name)
            self.generation_model = AutoModelForCausalLM.from_pretrained(generation_model_name)
            self.generation_model = self.generation_model.to(self.device)
            self.generation_model.eval()
            
            print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def predict_sentiment(self, text):
        """
        é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ
        
        Args:
            text (str): è¾“å…¥æ–‡æœ¬
        
        Returns:
            tuple: (sentiment, confidence) - æƒ…æ„Ÿæ ‡ç­¾å’Œç½®ä¿¡åº¦
        """
        try:
            # ç¼–ç æ–‡æœ¬
            inputs = self.sentiment_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            )
            
            # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # é¢„æµ‹
            with torch.no_grad():
                outputs = self.sentiment_model(**inputs)
                prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(prediction, dim=-1).item()
                confidence = prediction[0][predicted_class].item()
            
            # è¿”å›ç»“æœ
            sentiment = "æ­£é¢" if predicted_class == 1 else "è´Ÿé¢"
            return sentiment, confidence
            
        except Exception as e:
            print(f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {e}")
            return "è´Ÿé¢", 0.5
    
    def generate_response(self, sentiment, original_text):
        """
        æ ¹æ®æƒ…æ„Ÿç”Ÿæˆå›å¤
        
        Args:
            sentiment (str): æƒ…æ„Ÿæ ‡ç­¾
            original_text (str): åŸå§‹æ–‡æœ¬
        
        Returns:
            str: ç”Ÿæˆçš„å›å¤
        """
        try:
            # æ ¹æ®æƒ…æ„Ÿé€‰æ‹©ä¸åŒçš„æç¤ºè¯æ¨¡æ¿
            if sentiment == "æ­£é¢":
                prompt = f"ç”¨æˆ·è¯´ï¼š{original_text}\nAIé¼“åŠ±å›å¤ï¼š"
            else:
                prompt = f"ç”¨æˆ·è¯´ï¼š{original_text}\nAIå®‰æ…°å›å¤ï¼š"
            
            # ç¼–ç è¾“å…¥
            inputs = self.generation_tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    inputs,
                    max_length=len(inputs[0]) + 50,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.generation_tokenizer.eos_token_id,
                    eos_token_id=self.generation_tokenizer.eos_token_id,
                    repetition_penalty=1.2  # å‡å°‘é‡å¤
                )
            
            # è§£ç è¾“å‡º
            generated_text = self.generation_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # å¤„ç†ä¸­æ–‡æ–‡æœ¬æ ¼å¼ï¼Œå»é™¤ä¸å¿…è¦çš„ç©ºæ ¼
            generated_text = generated_text.replace(" ", "")
            
            # æå–AIå›å¤éƒ¨åˆ†
            if "AIé¼“åŠ±å›å¤ï¼š" in generated_text:
                reply = generated_text.split("AIé¼“åŠ±å›å¤ï¼š")[1].strip()
            elif "AIå®‰æ…°å›å¤ï¼š" in generated_text:
                reply = generated_text.split("AIå®‰æ…°å›å¤ï¼š")[1].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œä½¿ç”¨å¤‡ç”¨å›å¤
                if sentiment == "æ­£é¢":
                    reply = "å¤ªå¥½äº†ï¼ä½ çš„æƒ³æ³•å¾ˆç§¯æï¼Œç»§ç»­ä¿æŒè¿™ç§å¿ƒæ€ï¼"
                else:
                    reply = "æˆ‘ç†è§£ä½ ç°åœ¨çš„æ„Ÿå—ã€‚æ¯ä¸ªäººéƒ½ä¼šæœ‰ä½è½çš„æ—¶å€™ï¼Œè¿™å¾ˆæ­£å¸¸ã€‚è®°ä½ï¼Œå›°éš¾æ€»æ˜¯æš‚æ—¶çš„ï¼Œä½ ä¸€å®šèƒ½åº¦è¿‡è¿™ä¸ªéš¾å…³ã€‚"
            
            return reply[:100]  # é™åˆ¶å›å¤é•¿åº¦
            
        except Exception as e:
            print(f"æ–‡æœ¬ç”Ÿæˆå‡ºé”™: {e}")
            # å¤‡ç”¨å›å¤
            if sentiment == "æ­£é¢":
                return "ä½ çš„æƒ³æ³•å¾ˆæ£’ï¼ç»§ç»­ä¿æŒç§¯æçš„å¿ƒæ€ï¼"
            else:
                return "æˆ‘èƒ½ç†è§£ä½ çš„æ„Ÿå—ã€‚è®°ä½ï¼Œä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥çš„ã€‚"

# åˆ›å»ºAIåŠ©æ‰‹å®ä¾‹
print("åˆå§‹åŒ–æƒ…æ„ŸAIåŠ©æ‰‹...")
ai_assistant = EmotionAI()

def analyze_and_reply(text):
    """
    åˆ†ææ–‡æœ¬æƒ…æ„Ÿå¹¶ç”Ÿæˆç›¸åº”å›å¤
    
    Args:
        text (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
    
    Returns:
        str: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœå’ŒAIå›å¤çš„æ–‡æœ¬
    """
    if not text or not text.strip():
        return "è¯·è¾“å…¥ä¸€äº›æ–‡æœ¬ï¼"
    
    try:
        # 1. æƒ…æ„Ÿåˆ¤æ–­
        sentiment, confidence = ai_assistant.predict_sentiment(text)
        
        # 2. ç”Ÿæˆå›å¤ï¼ˆæ­£é¢æ—¶ç”Ÿæˆé¼“åŠ±ï¼Œè´Ÿé¢æ—¶ç”Ÿæˆå®‰æ…°ï¼‰
        ai_reply = ai_assistant.generate_response(sentiment, text)
        
        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ” æƒ…æ„Ÿåˆ¤æ–­ï¼š{sentiment} (ç½®ä¿¡åº¦: {confidence:.2f})\n\nğŸ¤– AIå›å¤ï¼š{ai_reply}"
        
        return result
        
    except Exception as e:
        return f"å¤„ç†å‡ºé”™: {e}"

# åˆ›å»ºæ›´å¤šç¤ºä¾‹
examples = [
    ["æˆ‘ä»Šå¤©è€ƒè¯•è€ƒå¾—å¾ˆå¥½ï¼Œå¤ªå¼€å¿ƒäº†ï¼"],
    ["ä»Šå¤©å¿ƒæƒ…å¾ˆç³Ÿç³•ï¼Œä»€ä¹ˆéƒ½ä¸é¡ºå¿ƒ"],
    ["åˆšåˆšå®Œæˆäº†ä¸€ä¸ªå¾ˆæ£’çš„é¡¹ç›®ï¼Œæ„Ÿè§‰å¾ˆæœ‰æˆå°±æ„Ÿ"],
    ["æˆ‘è§‰å¾—å¾ˆæ²®ä¸§ï¼Œæ„Ÿè§‰è‡ªå·±ä»€ä¹ˆéƒ½åšä¸å¥½"],
    ["å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…ä¹Ÿå˜å¾—æ„‰å¿«èµ·æ¥"],
    ["å·¥ä½œå‹åŠ›å¤ªå¤§äº†ï¼Œæˆ‘å¿«å—ä¸äº†äº†"]
]

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """åˆ›å»ºGradioç½‘é¡µç•Œé¢"""
    
    interface = gr.Interface(
        fn=analyze_and_reply,
        inputs=gr.Textbox(
            label="è¾“å…¥ä½ çš„æ–‡æœ¬", 
            placeholder="è¯·è¾“å…¥ä½ æƒ³è¡¨è¾¾çš„å†…å®¹...",
            lines=3
        ),
        outputs=gr.Textbox(
            label="åˆ†æç»“æœ", 
            lines=6
        ),
        title="ğŸ¤– æƒ…æ„ŸAIåŠ©æ‰‹",
        description="""
        è¿™ä¸ªAIåŠ©æ‰‹å¯ä»¥ï¼š
        1. ğŸ“Š åˆ†æä½ çš„æ–‡æœ¬æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
        2. ğŸ’¬ æ ¹æ®æƒ…æ„Ÿæä¾›ç›¸åº”çš„å›å¤ï¼ˆé¼“åŠ±æˆ–å®‰æ…°ï¼‰
        
        è¯•ç€è¾“å…¥ä¸€äº›æ–‡æœ¬ï¼Œçœ‹çœ‹AIå¦‚ä½•å›åº”ï¼
        """,
        examples=examples,
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            font-family: 'Arial', sans-serif;
        }
        .gr-button {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 15px 32px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 8px;
        }
        """,
        allow_flagging="never"
    )
    
    return interface

if __name__ == "__main__":
    print("å¯åŠ¨Gradioç½‘é¡µç•Œé¢...")
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        share=True,  # åˆ›å»ºå…¬å…±é“¾æ¥
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,  # æŒ‡å®šç«¯å£
        show_error=True  # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
    )
