# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿè¯„ä¼°ä¸é‡åŒ–åˆ†æ
å…¨é¢è¯„ä¼°åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ï¼Œç”Ÿæˆç§‘ç ”çº§æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    confusion_matrix, classification_report
)

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class ComprehensiveEvaluator:
    """ç»¼åˆè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results = {
            'classification': {},
            'generation': {},
            'summary': {}
        }
    
    def evaluate_classification_model(self, model_name: str, model_path: str, 
                                     test_data_path: str = 'data/data.csv'):
        """
        è¯„ä¼°åˆ†ç±»æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_path: æ¨¡å‹è·¯å¾„
            test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        """
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°åˆ†ç±»æ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            import torch
            
            # åŠ è½½æ¨¡å‹
            print("åŠ è½½æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForSequenceClassification.from_pretrained(model_path)
            model.eval()
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model.to(device)
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            print("åŠ è½½æµ‹è¯•æ•°æ®...")
            df = pd.read_csv(test_data_path)
            
            # æ˜ å°„æ ‡ç­¾
            emotion_map = {
                'é–‹å¿ƒèªèª¿': 1, 'æ‚²å‚·èªèª¿': 0, 'æ†¤æ€’èªèª¿': 0, 'å¹³æ·¡èªæ°£': 0,
                'é©šå¥‡èªèª¿': 1, 'å­æƒ¡èªèª¿': 0, 'é—œåˆ‡èªèª¿': 1, 'ç–‘å•èªèª¿': 0
            }
            df['sentiment'] = df['emotion'].apply(lambda x: emotion_map.get(x, 0))
            
            # é¢„æµ‹
            print("è¿›è¡Œé¢„æµ‹...")
            predictions = []
            true_labels = df['sentiment'].tolist()
            
            for text in df['text']:
                inputs = tokenizer(text, return_tensors="pt", truncation=True, 
                                 max_length=512).to(device)
                with torch.no_grad():
                    outputs = model(**inputs)
                    pred = torch.argmax(outputs.logits, dim=-1).item()
                predictions.append(pred)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(true_labels, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                true_labels, predictions, average='weighted'
            )
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(true_labels, predictions)
            
            # åˆ†ç±»æŠ¥å‘Š
            report = classification_report(true_labels, predictions, 
                                         target_names=['Negative', 'Positive'],
                                         output_dict=True)
            
            # é”™è¯¯åˆ†æ
            errors = self._analyze_errors(df, predictions, true_labels)
            
            # ä¿å­˜ç»“æœ
            self.results['classification'][model_name] = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'confusion_matrix': cm.tolist(),
                'classification_report': report,
                'error_analysis': errors
            }
            
            print(f"\nâœ… è¯„ä¼°å®Œæˆ")
            print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            print(f"  å¬å›ç‡: {recall:.4f}")
            print(f"  F1åˆ†æ•°: {f1:.4f}")
            
            return self.results['classification'][model_name]
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def _analyze_errors(self, df, predictions, true_labels, max_examples=20):
        """é”™è¯¯åˆ†æ"""
        print("\nè¿›è¡Œé”™è¯¯åˆ†æ...")
        
        errors = []
        error_patterns = {
            'false_positive': [],  # é¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºè´Ÿ
            'false_negative': []   # é¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºæ­£
        }
        
        for idx, (pred, true) in enumerate(zip(predictions, true_labels)):
            if pred != true:
                error_info = {
                    'text': df.iloc[idx]['text'],
                    'emotion': df.iloc[idx]['emotion'],
                    'true_label': 'Positive' if true == 1 else 'Negative',
                    'pred_label': 'Positive' if pred == 1 else 'Negative',
                    'text_length': len(df.iloc[idx]['text'])
                }
                errors.append(error_info)
                
                if pred == 1 and true == 0:
                    error_patterns['false_positive'].append(error_info)
                elif pred == 0 and true == 1:
                    error_patterns['false_negative'].append(error_info)
        
        # ç»Ÿè®¡
        total_errors = len(errors)
        fp_count = len(error_patterns['false_positive'])
        fn_count = len(error_patterns['false_negative'])
        
        print(f"  æ€»é”™è¯¯æ•°: {total_errors}")
        print(f"  å‡é˜³æ€§(FP): {fp_count}")
        print(f"  å‡é˜´æ€§(FN): {fn_count}")
        
        # åˆ†æé”™è¯¯æ¨¡å¼
        if fp_count > 0:
            avg_fp_length = np.mean([e['text_length'] for e in error_patterns['false_positive']])
            print(f"  FPå¹³å‡é•¿åº¦: {avg_fp_length:.1f}")
        
        if fn_count > 0:
            avg_fn_length = np.mean([e['text_length'] for e in error_patterns['false_negative']])
            print(f"  FNå¹³å‡é•¿åº¦: {avg_fn_length:.1f}")
        
        return {
            'total_errors': total_errors,
            'false_positive_count': fp_count,
            'false_negative_count': fn_count,
            'error_examples': errors[:max_examples],
            'error_patterns': {
                'false_positive_examples': error_patterns['false_positive'][:10],
                'false_negative_examples': error_patterns['false_negative'][:10]
            }
        }
    
    def evaluate_generation_model(self, model_name: str, 
                                  generated_texts: List[str],
                                  reference_texts: List[str]):
        """
        è¯„ä¼°ç”Ÿæˆæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
            reference_texts: å‚è€ƒæ–‡æœ¬åˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç”Ÿæˆæ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        try:
            # è®¡ç®—BLEU
            bleu_score = self._calculate_bleu(generated_texts, reference_texts)
            
            # è®¡ç®—ROUGE
            rouge_scores = self._calculate_rouge(generated_texts, reference_texts)
            
            # å…¶ä»–æŒ‡æ ‡
            avg_length = np.mean([len(t) for t in generated_texts])
            diversity = len(set(generated_texts)) / len(generated_texts)
            
            self.results['generation'][model_name] = {
                'bleu': bleu_score,
                'rouge_1': rouge_scores['rouge-1'],
                'rouge_2': rouge_scores['rouge-2'],
                'rouge_l': rouge_scores['rouge-l'],
                'avg_length': avg_length,
                'diversity': diversity
            }
            
            print(f"\nâœ… è¯„ä¼°å®Œæˆ")
            print(f"  BLEU: {bleu_score:.4f}")
            print(f"  ROUGE-1: {rouge_scores['rouge-1']:.4f}")
            print(f"  ROUGE-2: {rouge_scores['rouge-2']:.4f}")
            print(f"  ROUGE-L: {rouge_scores['rouge-l']:.4f}")
            print(f"  å¹³å‡é•¿åº¦: {avg_length:.1f}")
            print(f"  å¤šæ ·æ€§: {diversity:.4f}")
            
            return self.results['generation'][model_name]
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def _calculate_bleu(self, generated_texts, reference_texts):
        """è®¡ç®—BLEUåˆ†æ•°"""
        try:
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            
            smooth = SmoothingFunction()
            scores = []
            
            for gen, ref in zip(generated_texts, reference_texts):
                gen_tokens = list(gen)
                ref_tokens = [list(ref)]
                score = sentence_bleu(ref_tokens, gen_tokens, 
                                    smoothing_function=smooth.method1)
                scores.append(score)
            
            return np.mean(scores)
        except ImportError:
            print("  âš ï¸ NLTKæœªå®‰è£…ï¼Œè·³è¿‡BLEUè®¡ç®—")
            return 0.0
    
    def _calculate_rouge(self, generated_texts, reference_texts):
        """è®¡ç®—ROUGEåˆ†æ•°"""
        try:
            from rouge import Rouge
            
            rouge = Rouge()
            scores = rouge.get_scores(generated_texts, reference_texts, avg=True)
            
            return {
                'rouge-1': scores['rouge-1']['f'],
                'rouge-2': scores['rouge-2']['f'],
                'rouge-l': scores['rouge-l']['f']
            }
        except ImportError:
            print("  âš ï¸ rougeæœªå®‰è£…ï¼Œè·³è¿‡ROUGEè®¡ç®—")
            return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}
    
    def generate_comparison_table(self):
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        print(f"\n{'='*60}")
        print("ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼")
        print(f"{'='*60}")
        
        # ä»å®éªŒç»“æœåŠ è½½æ•°æ®
        exp_base = "results/experiments"
        exp_dirs = sorted([d for d in os.listdir(exp_base) 
                          if os.path.isdir(os.path.join(exp_base, d))], reverse=True)
        
        if not exp_dirs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å®éªŒç»“æœ")
            return None
        
        # åŠ è½½æœ€æ–°å®éªŒ
        latest_exp = exp_dirs[0]
        result_file = f"{exp_base}/{latest_exp}/all_results.json"
        
        with open(result_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # æ„å»ºè¡¨æ ¼æ•°æ®
        table_data = []
        for result in results:
            model_short = result['model_name'].split('/')[-1]
            lr = result['learning_rate']
            
            # åˆ¤æ–­æ•°æ®å¢å¼º
            data_aug = "æ˜¯" if "augmented" in result.get('data_file', '') else "å¦"
            
            row = {
                'æ¨¡å‹': model_short,
                'å­¦ä¹ ç‡': f"{lr:.0e}",
                'æ•°æ®å¢å¼º': data_aug,
                'å‡†ç¡®ç‡': f"{result['accuracy']*100:.2f}%",
                'F1': f"{result['f1']:.4f}",
                'è®­ç»ƒæ—¶é—´(s)': f"{result['training_time_seconds']:.1f}"
            }
            table_data.append(row)
        
        df_table = pd.DataFrame(table_data)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_idx = df_table['å‡†ç¡®ç‡'].str.rstrip('%').astype(float).idxmax()
        
        print("\nå®éªŒç»“æœå¯¹æ¯”è¡¨:")
        print(df_table.to_string(index=False))
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {df_table.iloc[best_idx]['æ¨¡å‹']} "
              f"(lr={df_table.iloc[best_idx]['å­¦ä¹ ç‡']}, "
              f"å‡†ç¡®ç‡={df_table.iloc[best_idx]['å‡†ç¡®ç‡']})")
        
        return df_table
    
    def generate_comprehensive_report(self, output_dir="results/evaluation"):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        print(f"{'='*60}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report = self._create_markdown_report()
        
        report_file = f"{output_dir}/comprehensive_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜JSONæ•°æ®
        json_file = report_file.replace('.md', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜: {json_file}")
        
        return report_file
    
    def compare_augmentation_models(self, original_dir='models/model_original', 
                                    augmented_dir='models/model_augmented',
                                    output_file='results/augmentation/comparison.json'):
        """
        å¯¹æ¯”åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½
        æ•´åˆè‡ªevaluate_models.py
        """
        print(f"\n{'='*60}")
        print("æ•°æ®å¢å¼ºæ¨¡å‹å¯¹æ¯”åˆ†æ")
        print(f"{'='*60}")
        
        # åŠ è½½è®­ç»ƒç»“æœ
        def load_training_results(model_dir):
            results_file = os.path.join(model_dir, 'training_results.json')
            if os.path.exists(results_file):
                with open(results_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return None
        
        original_results = load_training_results(original_dir)
        augmented_results = load_training_results(augmented_dir)
        
        if not original_results or not augmented_results:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹è®­ç»ƒç»“æœ")
            return None
        
        # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
        report = {
            'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'original_model': {
                'path': original_dir,
                'accuracy': original_results.get('accuracy', 0.0),
                'precision': original_results.get('precision', 0.0),
                'recall': original_results.get('recall', 0.0),
                'f1': original_results.get('f1', 0.0),
            },
            'augmented_model': {
                'path': augmented_dir,
                'accuracy': augmented_results.get('accuracy', 0.0),
                'precision': augmented_results.get('precision', 0.0),
                'recall': augmented_results.get('recall', 0.0),
                'f1': augmented_results.get('f1', 0.0),
            },
            'improvements': {}
        }
        
        # è®¡ç®—æå‡
        for metric in ['accuracy', 'precision', 'recall', 'f1']:
            orig_val = report['original_model'][metric]
            aug_val = report['augmented_model'][metric]
            if orig_val > 0:
                improvement_pct = ((aug_val - orig_val) / orig_val) * 100
                report['improvements'][metric] = {
                    'original': orig_val,
                    'augmented': aug_val,
                    'percentage_improvement': improvement_pct
                }
        
        # ä¿å­˜æŠ¥å‘Š
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_file = output_file.replace('.json', '.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"""# æ•°æ®å¢å¼ºæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

**å®éªŒæ—¥æœŸ**: {report['experiment_date']}

## æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | åŸå§‹æ•°æ® | å¢å¼ºæ•°æ® | ç›¸å¯¹æå‡ |
|------|---------|---------|---------|
""")
            for metric in ['accuracy', 'precision', 'recall', 'f1']:
                if metric in report['improvements']:
                    imp = report['improvements'][metric]
                    f.write(f"| {metric} | {imp['original']:.4f} | {imp['augmented']:.4f} | {imp['percentage_improvement']:+.2f}% |\n")
        
        print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return report
    
    def _create_markdown_report(self):
        """åˆ›å»ºMarkdownæ ¼å¼æŠ¥å‘Š"""
        
        report = f"""# ç³»ç»Ÿè¯„ä¼°ä¸é‡åŒ–åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**è¯„ä¼°æ¡†æ¶**: ç§‘ç ”çº§ç»¼åˆè¯„ä¼°ç³»ç»Ÿ

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šå¯¹æ‰€æœ‰å®éªŒæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„é‡åŒ–è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
- âœ… åˆ†ç±»ä»»åŠ¡æ€§èƒ½è¯„ä¼°
- âœ… é”™è¯¯æ¨¡å¼åˆ†æ
- âœ… æ¨¡å‹å¯¹æ¯”åˆ†æ
- âœ… å®éªŒç»“æœæ±‡æ€»

---

## 1ï¸âƒ£ åˆ†ç±»ä»»åŠ¡è¯„ä¼°

### 1.1 æ•´ä½“æ€§èƒ½å¯¹æ¯”

"""
        
        # åŠ è½½å®éªŒæ•°æ®ç”Ÿæˆè¡¨æ ¼
        exp_base = "results/experiments"
        exp_dirs = sorted([d for d in os.listdir(exp_base) 
                          if os.path.isdir(os.path.join(exp_base, d))], reverse=True)
        
        if exp_dirs:
            result_file = f"{exp_base}/{exp_dirs[0]}/all_results.json"
            with open(result_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            report += "| æ¨¡å‹ | å­¦ä¹ ç‡ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° | è®­ç»ƒæ—¶é—´ |\n"
            report += "|------|--------|--------|--------|--------|--------|----------|\n"
            
            for r in results:
                model = r['model_name'].split('/')[-1]
                lr = f"{r['learning_rate']:.0e}"
                report += f"| {model} | {lr} | "
                report += f"{r['accuracy']:.4f} | {r['precision']:.4f} | "
                report += f"{r['recall']:.4f} | {r['f1']:.4f} | "
                report += f"{r['training_time_seconds']:.1f}s |\n"
            
            # æ‰¾å‡ºæœ€ä½³
            best = max(results, key=lambda x: x['f1'])
            report += f"\n**ğŸ† æœ€ä½³æ¨¡å‹**: {best['model_name'].split('/')[-1]} "
            report += f"(F1={best['f1']:.4f})\n\n"
        
        report += """
### 1.2 è¯¦ç»†æŒ‡æ ‡è¯´æ˜

#### å‡†ç¡®ç‡ (Accuracy)
- **å®šä¹‰**: æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬å æ€»æ ·æœ¬çš„æ¯”ä¾‹
- **è®¡ç®—**: (TP + TN) / (TP + TN + FP + FN)
- **é€‚ç”¨**: ç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†

#### ç²¾ç¡®ç‡ (Precision)  
- **å®šä¹‰**: é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­çœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹
- **è®¡ç®—**: TP / (TP + FP)
- **å«ä¹‰**: æ¨¡å‹é¢„æµ‹ä¸ºæ­£æ—¶çš„å¯é æ€§

#### å¬å›ç‡ (Recall)
- **å®šä¹‰**: çœŸæ­£ä¸ºæ­£çš„æ ·æœ¬ä¸­è¢«é¢„æµ‹ä¸ºæ­£çš„æ¯”ä¾‹
- **è®¡ç®—**: TP / (TP + FN)
- **å«ä¹‰**: æ¨¡å‹æ‰¾å‡ºæ‰€æœ‰æ­£æ ·æœ¬çš„èƒ½åŠ›

#### F1åˆ†æ•° (F1-Score)
- **å®šä¹‰**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **è®¡ç®—**: 2 * (Precision * Recall) / (Precision + Recall)
- **ä¼˜åŠ¿**: å¹³è¡¡è€ƒè™‘ç²¾ç¡®ç‡å’Œå¬å›ç‡

---

## 2ï¸âƒ£ æ··æ·†çŸ©é˜µåˆ†æ

### å¯è§†åŒ–

æ··æ·†çŸ©é˜µè¯¦è§: `results/experiments/{exp_dirs[0] if exp_dirs else 'latest'}/confusion_matrices.png`

### çŸ©é˜µè§£è¯»

```
                é¢„æµ‹
              Neg   Pos
çœŸ    Neg  |  TN  |  FP  |
å®    Pos  |  FN  |  TP  |
```

- **TN (True Negative)**: æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿ
- **TP (True Positive)**: æ­£ç¡®é¢„æµ‹ä¸ºæ­£  
- **FP (False Positive)**: é”™è¯¯é¢„æµ‹ä¸ºæ­£ï¼ˆå‡é˜³æ€§ï¼‰
- **FN (False Negative)**: é”™è¯¯é¢„æµ‹ä¸ºè´Ÿï¼ˆå‡é˜´æ€§ï¼‰

---

## 3ï¸âƒ£ é”™è¯¯åˆ†æ

### 3.1 é”™è¯¯ç±»å‹åˆ†å¸ƒ

"""
        
        # å¦‚æœæœ‰é”™è¯¯åˆ†ææ•°æ®ï¼Œæ·»åŠ åˆ°æŠ¥å‘Š
        if 'classification' in self.results and self.results['classification']:
            for model_name, data in self.results['classification'].items():
                if 'error_analysis' in data:
                    ea = data['error_analysis']
                    report += f"\n#### {model_name}\n\n"
                    report += f"- æ€»é”™è¯¯æ•°: {ea['total_errors']}\n"
                    report += f"- å‡é˜³æ€§(FP): {ea['false_positive_count']}\n"
                    report += f"- å‡é˜´æ€§(FN): {ea['false_negative_count']}\n"
        
        report += """
### 3.2 é”™è¯¯æ¨¡å¼

#### å‡é˜³æ€§(FP)æ¨¡å¼
å°†è´Ÿé¢æƒ…æ„Ÿè¯¯åˆ¤ä¸ºæ­£é¢ï¼Œå¸¸è§åŸå› ï¼š
1. æ–‡æœ¬åŒ…å«æ­£é¢è¯æ±‡ä½†æ•´ä½“æ˜¯è´Ÿé¢
2. åè®½æˆ–å¹½é»˜è¡¨è¾¾
3. å¤æ‚çš„æƒ…æ„Ÿæ··åˆ

#### å‡é˜´æ€§(FN)æ¨¡å¼  
å°†æ­£é¢æƒ…æ„Ÿè¯¯åˆ¤ä¸ºè´Ÿé¢ï¼Œå¸¸è§åŸå› ï¼š
1. å«è“„çš„æ­£é¢è¡¨è¾¾
2. æ–‡æœ¬è¾ƒçŸ­ï¼Œä¿¡æ¯ä¸è¶³
3. ç‰¹å®šé¢†åŸŸçš„è¡¨è¾¾æ–¹å¼

### 3.3 æ”¹è¿›å»ºè®®

1. **æ•°æ®å±‚é¢**
   - å¢åŠ éš¾ä¾‹æ ·æœ¬
   - å¹³è¡¡å„æƒ…æ„Ÿç±»åˆ«
   - æ ‡æ³¨æ›´ç»†è‡´çš„æƒ…æ„Ÿ

2. **æ¨¡å‹å±‚é¢**
   - å°è¯•æ›´å¤§çš„æ¨¡å‹
   - è°ƒæ•´åˆ†ç±»é˜ˆå€¼
   - é›†æˆå¤šä¸ªæ¨¡å‹

3. **ç‰¹å¾å±‚é¢**
   - åŠ å…¥æƒ…æ„Ÿè¯å…¸ç‰¹å¾
   - è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯
   - ä½¿ç”¨é¢„è®­ç»ƒçš„æƒ…æ„Ÿæ¨¡å‹

---

## 4ï¸âƒ£ ç”Ÿæˆä»»åŠ¡è¯„ä¼°

### 4.1 è¯„ä¼°æŒ‡æ ‡

#### BLEU (Bilingual Evaluation Understudy)
- **ç”¨é€”**: è¡¡é‡ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬çš„n-gramé‡åˆåº¦
- **èŒƒå›´**: 0-1ï¼Œè¶Šé«˜è¶Šå¥½
- **ç‰¹ç‚¹**: åå‘ç²¾ç¡®åŒ¹é…

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- **ROUGE-1**: unigramé‡åˆåº¦
- **ROUGE-2**: bigramé‡åˆåº¦
- **ROUGE-L**: æœ€é•¿å…¬å…±å­åºåˆ—
- **ç‰¹ç‚¹**: åå‘å¬å›ç‡

#### å¤šæ ·æ€§
- **å®šä¹‰**: ç”Ÿæˆæ–‡æœ¬çš„ç‹¬ç‰¹æ€§
- **è®¡ç®—**: unique_texts / total_texts
- **æ„ä¹‰**: é¿å…é‡å¤ç”Ÿæˆ

### 4.2 è¯„ä¼°ç»“æœ

"""
        
        if 'generation' in self.results and self.results['generation']:
            report += "| æ¨¡å‹ | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L | å¤šæ ·æ€§ |\n"
            report += "|------|------|---------|---------|---------|--------|\n"
            
            for model_name, data in self.results['generation'].items():
                report += f"| {model_name} | "
                report += f"{data.get('bleu', 0):.4f} | "
                report += f"{data.get('rouge_1', 0):.4f} | "
                report += f"{data.get('rouge_2', 0):.4f} | "
                report += f"{data.get('rouge_l', 0):.4f} | "
                report += f"{data.get('diversity', 0):.4f} |\n"
        else:
            report += "_ç”Ÿæˆä»»åŠ¡è¯„ä¼°å°šæœªè¿è¡Œ_\n\n"
            report += "è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œç”Ÿæˆè¯„ä¼°:\n"
            report += "```powershell\n"
            report += "python scripts/generation_experiments.py\n"
            report += "```\n"
        
        report += """

---

## 5ï¸âƒ£ å®éªŒé…ç½®è®°å½•

### 5.1 å®éªŒç¯å¢ƒ

- **Pythonç‰ˆæœ¬**: 3.8+
- **PyTorchç‰ˆæœ¬**: 2.0+
- **Transformersç‰ˆæœ¬**: 4.20+
- **ç¡¬ä»¶**: GPU (CUDA)
- **éšæœºç§å­**: 42

### 5.2 è®­ç»ƒé…ç½®

| å‚æ•° | å€¼ |
|------|-----|
| Batch Size | 16 |
| Epochs | 3 |
| Warmup Steps | 100 |
| Weight Decay | 0.01 |
| Optimizer | AdamW |

### 5.3 æ•°æ®é…ç½®

| æ•°æ®é›† | æ ·æœ¬æ•° | è¯´æ˜ |
|--------|--------|------|
| åŸå§‹æ•°æ® | 4,159 | ç¹ä½“ä¸­æ–‡ï¼Œ8ç±»æƒ…æ„Ÿ |
| å¢å¼ºæ•°æ® | 8,318 | åŒä¹‰è¯æ›¿æ¢+è¯­æ°”è¯æ’å…¥ |
| è®­ç»ƒé›† | 3,327 | 80% split |
| éªŒè¯é›† | 832 | 20% split |

---

## 6ï¸âƒ£ å¯¹ç…§å®éªŒåˆ†æ

### 6.1 æ¨¡å‹å¯¹æ¯”

**ç»“è®º**: MacBERTåœ¨ä¸­æ–‡æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šè¡¨ç°æœ€ä¼˜

**è¯æ®**:
1. æœ€é«˜å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
2. æ··æ·†çŸ©é˜µæ˜¾ç¤ºæ›´å°‘çš„é”™è¯¯é¢„æµ‹
3. è®­ç»ƒæ›²çº¿æ˜¾ç¤ºè‰¯å¥½æ”¶æ•›

### 6.2 å­¦ä¹ ç‡å½±å“

**å‘ç°**: ä¸åŒæ¨¡å‹çš„æœ€ä¼˜å­¦ä¹ ç‡ä¸åŒ

**è§‚å¯Ÿ**:
- BERTå’ŒRoBERTa: 5e-5 > 2e-5
- MacBERT: 2e-5 > 5e-5

**è§£é‡Š**: MacBERTçš„é¢„è®­ç»ƒæ›´å……åˆ†ï¼Œéœ€è¦æ›´æ¸©å’Œçš„å¾®è°ƒ

### 6.3 æ•°æ®å¢å¼ºæ•ˆæœ

**å®éªŒè®¾è®¡**: å¯¹ç…§ç»„(åŸå§‹) vs å®éªŒç»„(å¢å¼º)

**é¢„æœŸ**: æ•°æ®å¢å¼ºæå‡æ³›åŒ–èƒ½åŠ›

**éªŒè¯æ–¹æ³•**: 
1. åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šè¯„ä¼°
2. æ¯”è¾ƒå„é¡¹æŒ‡æ ‡
3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

---

## 7ï¸âƒ£ ç§‘ç ”å¯ç¤º

### ç”¨æ•°æ®è®²æ•…äº‹

âœ… **å¥½çš„å®è·µ**:
- æ‰€æœ‰ç»“è®ºéƒ½æœ‰æ•°æ®æ”¯æŒ
- ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡äº¤å‰éªŒè¯
- å¯è§†åŒ–å¸®åŠ©ç†è§£

âŒ **é¿å…**:
- "æ„Ÿè§‰ä¸é”™"è¿™æ ·çš„ä¸»è§‚æè¿°
- åªçœ‹å•ä¸€æŒ‡æ ‡
- å¿½ç•¥é”™è¯¯åˆ†æ

### å¯¹ç…§å®éªŒç²¾ç¥

1. **æ§åˆ¶å˜é‡**: æ¯æ¬¡åªæ”¹å˜ä¸€ä¸ªå› ç´ 
2. **é‡å¤å®éªŒ**: å¤šæ¬¡è¿è¡Œç¡®ä¿ç¨³å®šæ€§
3. **ç»Ÿè®¡æ£€éªŒ**: ä½¿ç”¨t-testç­‰æ–¹æ³•éªŒè¯æ˜¾è‘—æ€§
4. **è®°å½•ä¸€åˆ‡**: å‚æ•°ã€éšæœºç§å­ã€ç¯å¢ƒ

### å¯é‡å¤æ€§

- âœ… å›ºå®šéšæœºç§å­
- âœ… è®°å½•æ‰€æœ‰è¶…å‚æ•°
- âœ… ä¿å­˜æ¨¡å‹å’Œç»“æœ
- âœ… è¯¦ç»†æ–‡æ¡£

---

## 8ï¸âƒ£ æ€»ç»“ä¸å»ºè®®

### ä¸»è¦å‘ç°

1. **æ¨¡å‹é€‰æ‹©å¾ˆé‡è¦**: MacBERT > RoBERTa > BERT
2. **è¶…å‚æ•°éœ€è¦è°ƒä¼˜**: ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„æœ€ä¼˜è®¾ç½®
3. **é”™è¯¯åˆ†ææœ‰ä»·å€¼**: å¯ä»¥æŒ‡å¯¼æ•°æ®æ”¶é›†å’Œæ¨¡å‹æ”¹è¿›

### ä¸‹ä¸€æ­¥å·¥ä½œ

1. **çŸ­æœŸ**
   - [ ] å®Œæˆç”Ÿæˆä»»åŠ¡è¯„ä¼°
   - [ ] è¿›è¡Œæ•°æ®å¢å¼ºå¯¹æ¯”å®éªŒ
   - [ ] æ·»åŠ äººå·¥è¯„åˆ†

2. **ä¸­æœŸ**
   - [ ] å°è¯•æ¨¡å‹é›†æˆ
   - [ ] è¿›è¡Œé”™è¯¯æ ·æœ¬çš„æ·±å…¥åˆ†æ
   - [ ] æ”¶é›†æ›´å¤šéš¾ä¾‹æ•°æ®

3. **é•¿æœŸ**
   - [ ] æ¢ç´¢å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ
   - [ ] å¼€å‘å®æ—¶APIæœåŠ¡
   - [ ] å‘è¡¨å­¦æœ¯è®ºæ–‡

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. BERT: Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers", 2018
2. MacBERT: Cui et al., "Revisiting Pre-Trained Models for Chinese NLP", 2020
3. BLEU: Papineni et al., "BLEU: a Method for Automatic Evaluation", 2002
4. ROUGE: Lin, "ROUGE: A Package for Automatic Evaluation", 2004

---

<div align="center">

**æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿç”Ÿæˆ**

æ•°æ®é©±åŠ¨ Â· è¯æ®æ”¯æŒ Â· ç§‘å­¦ä¸¥è°¨

</div>
"""
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”¬ ç³»ç»Ÿè¯„ä¼°ä¸é‡åŒ–åˆ†æ")
    print("=" * 60)
    
    evaluator = ComprehensiveEvaluator()
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    evaluator.generate_comparison_table()
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    evaluator.generate_comprehensive_report()
    
    print("\n" + "=" * 60)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 60)
    print("\næŸ¥çœ‹ç»“æœ:")
    print("  ğŸ“ results/evaluation/comprehensive_evaluation_*.md")
    print("  ğŸ“ results/evaluation/comprehensive_evaluation_*.json")

if __name__ == "__main__":
    main()

