# -*- coding: utf-8 -*-
"""
åˆ†ç±»æ¨¡å‹ä¼˜åŒ–å®éªŒ
å¯¹æ¯”ä¸åŒé¢„è®­ç»ƒæ¨¡å‹å’Œè¶…å‚æ•°é…ç½®
"""

import pandas as pd
import torch
import os
import json
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    Trainer, 
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# æ£€æŸ¥GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

class SentimentDataset(Dataset):
    """æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class LossHistoryCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„loss"""
    def __init__(self):
        self.train_losses = []
        self.eval_losses = []
        self.steps = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            if 'loss' in logs:
                self.train_losses.append(logs['loss'])
                self.steps.append(state.global_step)
            if 'eval_loss' in logs:
                self.eval_losses.append(logs['eval_loss'])

def map_emotion_to_sentiment(emotion):
    """æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„"""
    emotion_map = {
        'é–‹å¿ƒèªèª¿': 1,
        'æ‚²å‚·èªèª¿': 0,
        'æ†¤æ€’èªèª¿': 0,
        'å¹³æ·¡èªæ°£': 0,
        'é©šå¥‡èªèª¿': 1,
        'å­æƒ¡èªèª¿': 0,
        'é—œåˆ‡èªèª¿': 1,
        'ç–‘å•èªèª¿': 0
    }
    return emotion_map.get(emotion, 0)

def load_data(data_file='data/data.csv'):
    """åŠ è½½æ•°æ®"""
    df = pd.read_csv(data_file)
    df['sentiment'] = df['emotion'].apply(map_emotion_to_sentiment)
    return df['text'].tolist(), df['sentiment'].tolist()

def compute_metrics(eval_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def train_model(model_name, learning_rate, data_file, output_dir, epochs=3):
    """
    è®­ç»ƒå•ä¸ªæ¨¡å‹é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°
        learning_rate: å­¦ä¹ ç‡
        data_file: æ•°æ®æ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
        epochs: è®­ç»ƒè½®æ•°
    
    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    print("\n" + "=" * 60)
    print(f"å®éªŒé…ç½®:")
    print(f"  æ¨¡å‹: {model_name}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    print("=" * 60)
    
    start_time = time.time()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2
        )
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # åŠ è½½æ•°æ®
    texts, labels = load_data(data_file)
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"è®­ç»ƒé›†: {len(train_texts)}, éªŒè¯é›†: {len(val_texts)}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)
    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=learning_rate,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir=os.path.join(output_dir, 'logs'),
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=100,
        save_steps=200,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to=None,
        fp16=torch.cuda.is_available(),
    )
    
    # Losså›è°ƒ
    loss_callback = LossHistoryCallback()
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[loss_callback]
    )
    
    # è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # è¯„ä¼°
    eval_results = trainer.evaluate()
    training_time = time.time() - start_time
    
    # è·å–é¢„æµ‹ç»“æœç”¨äºæ··æ·†çŸ©é˜µ
    predictions = trainer.predict(val_dataset)
    pred_labels = np.argmax(predictions.predictions, axis=1)
    cm = confusion_matrix(val_labels, pred_labels)
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs(output_dir, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ç»“æœ
    results = {
        'model_name': model_name,
        'learning_rate': learning_rate,
        'epochs': epochs,
        'train_size': len(train_texts),
        'val_size': len(val_texts),
        'accuracy': float(eval_results['eval_accuracy']),
        'precision': float(eval_results['eval_precision']),
        'recall': float(eval_results['eval_recall']),
        'f1': float(eval_results['eval_f1']),
        'training_time_seconds': training_time,
        'train_losses': loss_callback.train_losses,
        'eval_losses': loss_callback.eval_losses,
        'steps': loss_callback.steps,
        'confusion_matrix': cm.tolist()
    }
    
    # ä¿å­˜ç»“æœ
    with open(os.path.join(output_dir, 'experiment_results.json'), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ!")
    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {results['f1']:.4f}")
    print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    
    return results

def run_all_experiments():
    """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
    
    # å®éªŒé…ç½®
    models = [
        'bert-base-chinese',
        'hfl/chinese-roberta-wwm-ext',
        'hfl/chinese-macbert-base'
    ]
    
    learning_rates = [2e-5, 5e-5]
    
    print("=" * 60)
    print("ğŸ”¬ åˆ†ç±»æ¨¡å‹ä¼˜åŒ–å®éªŒ")
    print("=" * 60)
    print(f"\nå°†æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹ Ã— {len(learning_rates)} ä¸ªå­¦ä¹ ç‡ = {len(models) * len(learning_rates)} ä¸ªé…ç½®")
    print(f"é¢„è®¡æ€»ç”¨æ—¶: çº¦ {len(models) * len(learning_rates) * 20} åˆ†é’Ÿ")
    
    response = input("\næ˜¯å¦ç»§ç»­? (y/n): ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆã€‚")
        return
    
    all_results = []
    experiment_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    for model_name in models:
        for lr in learning_rates:
            # ç”Ÿæˆè¾“å‡ºç›®å½•å
            model_short = model_name.split('/')[-1]
            lr_str = f"{lr:.0e}".replace('-', '_')
            output_dir = f"models/experiments/{experiment_id}/{model_short}_lr{lr_str}"
            
            print(f"\n{'='*60}")
            print(f"å®éªŒ {len(all_results) + 1}/{len(models) * len(learning_rates)}")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            result = train_model(
                model_name=model_name,
                learning_rate=lr,
                data_file='data/data.csv',
                output_dir=output_dir,
                epochs=3
            )
            
            if result:
                all_results.append(result)
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(2)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(all_results, experiment_id)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
    plot_experiment_results(all_results, experiment_id)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")
    print("=" * 60)
    print(f"\nç»“æœä¿å­˜åœ¨: results/experiments/{experiment_id}/")

def generate_comparison_report(results, experiment_id):
    """ç”Ÿæˆå®éªŒå¯¹æ¯”æŠ¥å‘Š"""
    
    output_dir = f"results/experiments/{experiment_id}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    with open(f"{output_dir}/all_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_content = f"""# åˆ†ç±»æ¨¡å‹ä¼˜åŒ–å®éªŒæŠ¥å‘Š

**å®éªŒID**: {experiment_id}  
**å®éªŒæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**å®éªŒé…ç½®æ•°**: {len(results)}

## ğŸ“Š å®éªŒç»“æœæ±‡æ€»

### æ€§èƒ½å¯¹æ¯”è¡¨

| æ¨¡å‹ | å­¦ä¹ ç‡ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° | è®­ç»ƒæ—¶é—´ |
|------|--------|--------|--------|--------|--------|----------|
"""
    
    for r in results:
        model_short = r['model_name'].split('/')[-1]
        md_content += f"| {model_short} | {r['learning_rate']:.0e} | "
        md_content += f"{r['accuracy']:.4f} | {r['precision']:.4f} | "
        md_content += f"{r['recall']:.4f} | {r['f1']:.4f} | {r['training_time_seconds']:.1f}s |\n"
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_acc = max(results, key=lambda x: x['accuracy'])
    best_f1 = max(results, key=lambda x: x['f1'])
    
    md_content += f"""
## ğŸ† æœ€ä½³é…ç½®

### æœ€é«˜å‡†ç¡®ç‡
- **æ¨¡å‹**: {best_acc['model_name']}
- **å­¦ä¹ ç‡**: {best_acc['learning_rate']:.0e}
- **å‡†ç¡®ç‡**: {best_acc['accuracy']:.4f}
- **F1åˆ†æ•°**: {best_acc['f1']:.4f}

### æœ€é«˜F1åˆ†æ•°
- **æ¨¡å‹**: {best_f1['model_name']}
- **å­¦ä¹ ç‡**: {best_f1['learning_rate']:.0e}
- **å‡†ç¡®ç‡**: {best_f1['accuracy']:.4f}
- **F1åˆ†æ•°**: {best_f1['f1']:.4f}

## ğŸ“ˆ å…³é”®å‘ç°

### æ¨¡å‹å¯¹æ¯”
"""
    
    # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
    model_stats = {}
    for r in results:
        model = r['model_name']
        if model not in model_stats:
            model_stats[model] = {'f1': [], 'acc': []}
        model_stats[model]['f1'].append(r['f1'])
        model_stats[model]['acc'].append(r['accuracy'])
    
    for model, stats in model_stats.items():
        avg_f1 = np.mean(stats['f1'])
        avg_acc = np.mean(stats['acc'])
        md_content += f"- **{model}**: å¹³å‡F1 = {avg_f1:.4f}, å¹³å‡å‡†ç¡®ç‡ = {avg_acc:.4f}\n"
    
    md_content += """
### å­¦ä¹ ç‡å½±å“
"""
    
    # æŒ‰å­¦ä¹ ç‡åˆ†ç»„
    lr_stats = {}
    for r in results:
        lr = r['learning_rate']
        if lr not in lr_stats:
            lr_stats[lr] = {'f1': [], 'acc': []}
        lr_stats[lr]['f1'].append(r['f1'])
        lr_stats[lr]['acc'].append(r['accuracy'])
    
    for lr, stats in sorted(lr_stats.items()):
        avg_f1 = np.mean(stats['f1'])
        avg_acc = np.mean(stats['acc'])
        md_content += f"- **LR = {lr:.0e}**: å¹³å‡F1 = {avg_f1:.4f}, å¹³å‡å‡†ç¡®ç‡ = {avg_acc:.4f}\n"
    
    md_content += f"""
## ğŸ’¡ ç»“è®ºä¸å»ºè®®

1. **æœ€ä½³æ¨¡å‹**: {best_f1['model_name']} åœ¨F1åˆ†æ•°ä¸Šè¡¨ç°æœ€å¥½
2. **æ¨èå­¦ä¹ ç‡**: æ ¹æ®å®éªŒç»“æœï¼Œ{'è¾ƒé«˜' if best_f1['learning_rate'] > 3e-5 else 'è¾ƒä½'}çš„å­¦ä¹ ç‡æ›´é€‚åˆæ­¤ä»»åŠ¡
3. **è®­ç»ƒæ•ˆç‡**: å¹³å‡è®­ç»ƒæ—¶é—´ä¸º {np.mean([r['training_time_seconds'] for r in results]):.1f} ç§’

## ğŸ“ å®éªŒæ–‡ä»¶

- å®Œæ•´ç»“æœ: `all_results.json`
- Lossæ›²çº¿: `loss_curves.png`
- æ€§èƒ½å¯¹æ¯”: `performance_comparison.png`
- æ··æ·†çŸ©é˜µ: `confusion_matrices.png`

## ğŸ”¬ ç§‘ç ”å»ºè®®

1. **å¯é‡å¤æ€§**: æ‰€æœ‰å®éªŒä½¿ç”¨å›ºå®šéšæœºç§å­(42)ï¼Œç¡®ä¿å¯é‡å¤
2. **å…¬å¹³å¯¹æ¯”**: æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ’åˆ†å’Œè®­ç»ƒå‚æ•°
3. **å¤šæŒ‡æ ‡è¯„ä¼°**: ä¸ä»…çœ‹å‡†ç¡®ç‡ï¼Œè¿˜éœ€å…³æ³¨F1ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
4. **è®­ç»ƒç¨³å®šæ€§**: è§‚å¯Ÿlossæ›²çº¿ï¼Œç¡®ä¿æ¨¡å‹æ”¶æ•›

---

*æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–å®éªŒç³»ç»Ÿç”Ÿæˆ*
"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open(f"{output_dir}/experiment_report.md", 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"\nâœ“ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {output_dir}/experiment_report.md")

def plot_experiment_results(results, experiment_id):
    """ç»˜åˆ¶å®éªŒç»“æœå›¾è¡¨"""
    
    output_dir = f"results/experiments/{experiment_id}"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Lossæ›²çº¿å¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    for idx, result in enumerate(results):
        ax = axes[idx // 2, idx % 2]
        model_short = result['model_name'].split('/')[-1]
        lr_str = f"{result['learning_rate']:.0e}"
        
        if result['steps'] and result['train_losses']:
            ax.plot(result['steps'], result['train_losses'], label='Train Loss', linewidth=2)
        
        ax.set_title(f"{model_short} (LR={lr_str})", fontsize=12, fontweight='bold')
        ax.set_xlabel('Steps')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/loss_curves.png", dpi=300, bbox_inches='tight')
    print(f"âœ“ Lossæ›²çº¿å·²ä¿å­˜: {output_dir}/loss_curves.png")
    plt.close()
    
    # 2. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    models = [r['model_name'].split('/')[-1] for r in results]
    lrs = [f"{r['learning_rate']:.0e}" for r in results]
    labels = [f"{m}\n{lr}" for m, lr in zip(models, lrs)]
    
    accuracies = [r['accuracy'] for r in results]
    f1_scores = [r['f1'] for r in results]
    
    x = np.arange(len(results))
    
    # å‡†ç¡®ç‡
    axes[0].bar(x, accuracies, color='steelblue', alpha=0.8)
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(labels, rotation=45, ha='right')
    axes[0].set_ylabel('å‡†ç¡®ç‡')
    axes[0].set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontweight='bold')
    axes[0].set_ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(accuracies):
        axes[0].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom', fontsize=9)
    
    # F1åˆ†æ•°
    axes[1].bar(x, f1_scores, color='coral', alpha=0.8)
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(labels, rotation=45, ha='right')
    axes[1].set_ylabel('F1åˆ†æ•°')
    axes[1].set_title('æ¨¡å‹F1åˆ†æ•°å¯¹æ¯”', fontweight='bold')
    axes[1].set_ylim([min(f1_scores) - 0.05, max(f1_scores) + 0.05])
    axes[1].grid(True, alpha=0.3, axis='y')
    
    for i, v in enumerate(f1_scores):
        axes[1].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/performance_comparison.png", dpi=300, bbox_inches='tight')
    print(f"âœ“ æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {output_dir}/performance_comparison.png")
    plt.close()
    
    # 3. æ··æ·†çŸ©é˜µ
    n_results = len(results)
    n_plots = min(n_results, 6)  # æœ€å¤šæ˜¾ç¤º6ä¸ª
    
    # è®¡ç®—å­å›¾å¸ƒå±€
    if n_plots <= 2:
        nrows, ncols = 1, n_plots
    elif n_plots <= 4:
        nrows, ncols = 2, 2
    else:
        nrows, ncols = 2, 3
    
    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows))
    
    # ç¡®ä¿axesæ˜¯2Dæ•°ç»„
    if n_plots == 1:
        axes = np.array([[axes]])
    elif nrows == 1:
        axes = axes.reshape(1, -1)
    
    for idx in range(n_plots):
        result = results[idx]
        ax = axes[idx // ncols, idx % ncols]
        model_short = result['model_name'].split('/')[-1]
        lr_str = f"{result['learning_rate']:.0e}"
        
        cm = np.array(result['confusion_matrix'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, 
                   xticklabels=['Negative', 'Positive'],
                   yticklabels=['Negative', 'Positive'])
        ax.set_title(f"{model_short} (LR={lr_str})", fontweight='bold')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(n_plots, nrows * ncols):
        axes[idx // ncols, idx % ncols].axis('off')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/confusion_matrices.png", dpi=300, bbox_inches='tight')
    print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {output_dir}/confusion_matrices.png")
    plt.close()

def regenerate_plots_only(experiment_id=None):
    """
    åªé‡æ–°ç”Ÿæˆå›¾è¡¨ï¼Œä¸é‡æ–°è®­ç»ƒ
    æ•´åˆè‡ªregenerate_plots.py
    """
    print("=" * 60)
    print("ğŸ¨ é‡æ–°ç”Ÿæˆå®éªŒå¯è§†åŒ–å›¾è¡¨")
    print("=" * 60)
    
    # æŸ¥æ‰¾å®éªŒç»“æœ
    exp_base = "results/experiments"
    if not os.path.exists(exp_base):
        print(f"âŒ æ‰¾ä¸åˆ°å®éªŒç›®å½•: {exp_base}")
        return
    
    # è·å–æ‰€æœ‰å®éªŒID
    exp_ids = [d for d in os.listdir(exp_base) if os.path.isdir(os.path.join(exp_base, d))]
    
    if not exp_ids:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
        return
    
    # ä½¿ç”¨æŒ‡å®šçš„å®éªŒæˆ–æœ€æ–°çš„å®éªŒ
    if experiment_id is None:
        exp_ids.sort(reverse=True)
        experiment_id = exp_ids[0]
    
    print(f"\nä½¿ç”¨å®éªŒID: {experiment_id}")
    
    # åŠ è½½ç»“æœ
    result_dir = f"{exp_base}/{experiment_id}"
    result_file = f"{result_dir}/all_results.json"
    
    if not os.path.exists(result_file):
        print(f"âŒ æ‰¾ä¸åˆ°å®éªŒç»“æœ: {result_file}")
        return
    
    with open(result_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    print(f"âœ“ åŠ è½½äº† {len(results)} ä¸ªå®éªŒç»“æœ")
    
    # ç”Ÿæˆå›¾è¡¨
    plot_experiment_results(results, experiment_id)
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼å›¾è¡¨å·²æ›´æ–°:")
    print("=" * 60)
    print(f"ğŸ“ {result_dir}/loss_curves.png")
    print(f"ğŸ“ {result_dir}/performance_comparison.png")
    print(f"ğŸ“ {result_dir}/confusion_matrices.png")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†ç±»æ¨¡å‹ä¼˜åŒ–å®éªŒ')
    parser.add_argument('--regenerate-only', action='store_true',
                       help='åªé‡æ–°ç”Ÿæˆå›¾è¡¨ï¼Œä¸é‡æ–°è®­ç»ƒ')
    parser.add_argument('--experiment-id', type=str, default=None,
                       help='æŒ‡å®šè¦é‡æ–°ç”Ÿæˆå›¾è¡¨çš„å®éªŒIDï¼ˆç”¨äº--regenerate-onlyï¼‰')
    
    args = parser.parse_args()
    
    if args.regenerate_only:
        regenerate_plots_only(args.experiment_id)
    else:
        run_all_experiments()

if __name__ == "__main__":
    main()

