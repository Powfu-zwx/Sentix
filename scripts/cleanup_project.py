#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®æ¸…ç†è„šæœ¬
åˆ é™¤é‡å¤å’Œæ— ç”¨å†…å®¹ï¼Œä¼˜åŒ–é¡¹ç›®æ¶æ„
"""

import os
import shutil
from pathlib import Path

def get_size_mb(path):
    """è·å–æ–‡ä»¶æˆ–ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
    if os.path.isfile(path):
        return os.path.getsize(path) / (1024 * 1024)
    
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.exists(fp):
                total += os.path.getsize(fp)
    return total / (1024 * 1024)

def cleanup_checkpoints(base_dir="models"):
    """åˆ é™¤æ‰€æœ‰checkpointæ–‡ä»¶å¤¹"""
    print("\n1ï¸âƒ£ æ¸…ç†Checkpointæ–‡ä»¶...")
    print("-" * 60)
    
    deleted_size = 0
    deleted_count = 0
    
    for root, dirs, files in os.walk(base_dir):
        for dirname in dirs:
            if dirname.startswith("checkpoint-"):
                checkpoint_path = os.path.join(root, dirname)
                size = get_size_mb(checkpoint_path)
                
                print(f"  ğŸ—‘ï¸  åˆ é™¤: {checkpoint_path} ({size:.1f} MB)")
                shutil.rmtree(checkpoint_path)
                
                deleted_size += size
                deleted_count += 1
    
    print(f"\n  âœ… åˆ é™¤äº† {deleted_count} ä¸ªcheckpoint")
    print(f"  ğŸ’¾ é‡Šæ”¾ç©ºé—´: {deleted_size:.1f} MB")
    
    return deleted_size

def cleanup_gradio_temp():
    """æ¸…ç†Gradioä¸´æ—¶æ–‡ä»¶"""
    print("\n2ï¸âƒ£ æ¸…ç†Gradioä¸´æ—¶æ–‡ä»¶...")
    print("-" * 60)
    
    gradio_dir = ".gradio"
    if os.path.exists(gradio_dir):
        size = get_size_mb(gradio_dir)
        print(f"  ğŸ—‘ï¸  åˆ é™¤: {gradio_dir}/ ({size:.1f} MB)")
        shutil.rmtree(gradio_dir)
        print(f"  âœ… å·²åˆ é™¤")
        return size
    else:
        print(f"  â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ä¸´æ—¶æ–‡ä»¶")
        return 0

def cleanup_pycache():
    """æ¸…ç†Pythonç¼“å­˜"""
    print("\n3ï¸âƒ£ æ¸…ç†Pythonç¼“å­˜...")
    print("-" * 60)
    
    deleted_size = 0
    deleted_count = 0
    
    for root, dirs, files in os.walk("."):
        if "__pycache__" in dirs:
            cache_path = os.path.join(root, "__pycache__")
            size = get_size_mb(cache_path)
            
            print(f"  ğŸ—‘ï¸  åˆ é™¤: {cache_path}")
            shutil.rmtree(cache_path)
            
            deleted_size += size
            deleted_count += 1
        
        # åˆ é™¤.pycæ–‡ä»¶
        for filename in files:
            if filename.endswith(".pyc"):
                file_path = os.path.join(root, filename)
                os.remove(file_path)
    
    print(f"  âœ… åˆ é™¤äº† {deleted_count} ä¸ªç¼“å­˜ç›®å½•")
    print(f"  ğŸ’¾ é‡Šæ”¾ç©ºé—´: {deleted_size:.1f} MB")
    
    return deleted_size

def cleanup_duplicate_models():
    """æ¸…ç†å®éªŒä¸­çš„é‡å¤æ¨¡å‹ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰"""
    print("\n4ï¸âƒ£ æ¸…ç†å®éªŒæ¨¡å‹...")
    print("-" * 60)
    
    exp_base = "models/experiments"
    if not os.path.exists(exp_base):
        print("  â„¹ï¸  æ²¡æœ‰å®éªŒæ¨¡å‹")
        return 0
    
    experiments = sorted([d for d in os.listdir(exp_base) 
                         if os.path.isdir(os.path.join(exp_base, d))], 
                        reverse=True)
    
    if len(experiments) <= 1:
        print(f"  â„¹ï¸  åªæœ‰ {len(experiments)} ä¸ªå®éªŒï¼Œä¿ç•™å…¨éƒ¨")
        return 0
    
    # ä¿ç•™æœ€æ–°çš„å®éªŒï¼Œåˆ é™¤æ—§çš„
    deleted_size = 0
    keep_exp = experiments[0]
    print(f"  âœ… ä¿ç•™æœ€æ–°å®éªŒ: {keep_exp}")
    
    for exp_id in experiments[1:]:
        exp_path = os.path.join(exp_base, exp_id)
        size = get_size_mb(exp_path)
        
        print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§å®éªŒ: {exp_id} ({size:.1f} MB)")
        shutil.rmtree(exp_path)
        deleted_size += size
    
    print(f"  ğŸ’¾ é‡Šæ”¾ç©ºé—´: {deleted_size:.1f} MB")
    
    return deleted_size

def optimize_results_structure():
    """ä¼˜åŒ–resultsç›®å½•ç»“æ„"""
    print("\n5ï¸âƒ£ ä¼˜åŒ–ç»“æœç›®å½•...")
    print("-" * 60)
    
    # ç§»åŠ¨é¡¶å±‚å¯è§†åŒ–åˆ°å¯è§†åŒ–å­ç›®å½•
    viz_dir = "results/visualizations"
    os.makedirs(viz_dir, exist_ok=True)
    
    files_to_move = [
        ("results/data_visualization.png", "results/visualizations/data_distribution.png"),
        ("results/augmentation_comparison.json", "results/augmentation/comparison.json"),
        ("results/augmentation_comparison.md", "results/augmentation/comparison.md"),
    ]
    
    for src, dst in files_to_move:
        if os.path.exists(src):
            os.makedirs(os.path.dirname(dst), exist_ok=True)
            shutil.move(src, dst)
            print(f"  ğŸ“ ç§»åŠ¨: {src} -> {dst}")
    
    print("  âœ… ç›®å½•ç»“æ„å·²ä¼˜åŒ–")

def create_readme_files():
    """ä¸ºå„ç›®å½•åˆ›å»ºREADME"""
    print("\n6ï¸âƒ£ åˆ›å»ºç›®å½•è¯´æ˜...")
    print("-" * 60)
    
    readmes = {
        "models/README.md": """# æ¨¡å‹ç›®å½•

## ç›®å½•è¯´æ˜

- `sentiment_model/` - è®­ç»ƒå¥½çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç”¨äºç”Ÿäº§ï¼‰
- `experiments/` - å®éªŒæ¨¡å‹ï¼ˆå¯¹æ¯”æµ‹è¯•ç”¨ï¼‰

## æ¨¡å‹æ–‡ä»¶

æ¯ä¸ªæ¨¡å‹ç›®å½•åŒ…å«ï¼š
- `model.safetensors` - æ¨¡å‹æƒé‡
- `config.json` - æ¨¡å‹é…ç½®
- `tokenizer.json` - åˆ†è¯å™¨
- `vocab.txt` - è¯è¡¨

## ä½¿ç”¨æ–¹æ³•

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('models/sentiment_model')
tokenizer = AutoTokenizer.from_pretrained('models/sentiment_model')
```
""",
        "results/README.md": """# å®éªŒç»“æœç›®å½•

## ç›®å½•ç»“æ„

- `visualizations/` - æ•°æ®å¯è§†åŒ–å›¾è¡¨
- `augmentation/` - æ•°æ®å¢å¼ºå¯¹æ¯”ç»“æœ
- `experiments/` - æ¨¡å‹å¯¹æ¯”å®éªŒç»“æœ
- `generation_experiments/` - ç”Ÿæˆæ¨¡å‹å®éªŒ
- `conditional_generation/` - æ¡ä»¶ç”Ÿæˆå®éªŒ

## æ–‡ä»¶è¯´æ˜

æ‰€æœ‰å®éªŒéƒ½ä¼šç”Ÿæˆï¼š
- JSONæ ¼å¼çš„ç»“æœæ•°æ®
- Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
- PNGæ ¼å¼çš„å¯è§†åŒ–å›¾è¡¨
"""
    }
    
    for path, content in readmes.items():
        if not os.path.exists(path):
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"  âœ… åˆ›å»º: {path}")

def generate_cleanup_report(total_saved):
    """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸…ç†æ€»ç»“")
    print("=" * 60)
    
    report = f"""
âœ… æ¸…ç†å®Œæˆï¼

ğŸ’¾ æ€»å…±é‡Šæ”¾ç©ºé—´: {total_saved:.1f} MB

ğŸ“ ä¼˜åŒ–åçš„ç›®å½•ç»“æ„:
```
project/
â”œâ”€â”€ data/              # æ•°æ®é›†
â”œâ”€â”€ docs/              # æ–‡æ¡£
â”œâ”€â”€ models/            # æ¨¡å‹ï¼ˆå·²åˆ é™¤checkpointï¼‰
â”‚   â”œâ”€â”€ sentiment_model/
â”‚   â””â”€â”€ experiments/   # åªä¿ç•™æœ€æ–°å®éªŒ
â”œâ”€â”€ results/           # ç»“æœï¼ˆå·²é‡ç»„ï¼‰
â”‚   â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ augmentation/
â”‚   â””â”€â”€ experiments/
â””â”€â”€ scripts/           # è„šæœ¬
```

ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:
1. è¿è¡Œ: git status æŸ¥çœ‹å˜åŒ–
2. å¦‚éœ€æ¢å¤checkpointï¼Œå¯ä»¥é‡æ–°è®­ç»ƒ
3. å®šæœŸè¿è¡Œæ­¤è„šæœ¬ä¿æŒé¡¹ç›®æ•´æ´
"""
    
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("CLEANUP_REPORT.txt", 'w', encoding='utf-8') as f:
        f.write(report)
    print("ğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: CLEANUP_REPORT.txt")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¹ é¡¹ç›®æ¸…ç†ä¸ä¼˜åŒ–")
    print("=" * 60)
    print("\nâš ï¸  æ­¤æ“ä½œå°†åˆ é™¤ï¼š")
    print("  - æ‰€æœ‰checkpointæ–‡ä»¶å¤¹")
    print("  - Gradioä¸´æ—¶æ–‡ä»¶")
    print("  - Pythonç¼“å­˜")
    print("  - æ—§çš„å®éªŒç»“æœï¼ˆä¿ç•™æœ€æ–°ï¼‰")
    
    response = input("\nç¡®è®¤ç»§ç»­? (y/n): ")
    if response.lower() != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    total_saved = 0
    
    # æ‰§è¡Œæ¸…ç†
    total_saved += cleanup_checkpoints()
    total_saved += cleanup_gradio_temp()
    total_saved += cleanup_pycache()
    total_saved += cleanup_duplicate_models()
    
    # ä¼˜åŒ–ç»“æ„
    optimize_results_structure()
    create_readme_files()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_cleanup_report(total_saved)

if __name__ == "__main__":
    main()

